{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8464061,"sourceType":"datasetVersion","datasetId":5045897},{"sourceId":8465237,"sourceType":"datasetVersion","datasetId":5046792}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\n# from torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\nimport timm\nimport tqdm\n\nfrom accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\nfrom accelerate.utils import set_seed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T02:29:25.366004Z","iopub.execute_input":"2024-05-22T02:29:25.366682Z","iopub.status.idle":"2024-05-22T02:29:29.513624Z","shell.execute_reply.started":"2024-05-22T02:29:25.366644Z","shell.execute_reply":"2024-05-22T02:29:29.512848Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_initialized()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:29.515287Z","iopub.execute_input":"2024-05-22T02:29:29.515736Z","iopub.status.idle":"2024-05-22T02:29:29.522750Z","shell.execute_reply.started":"2024-05-22T02:29:29.515709Z","shell.execute_reply":"2024-05-22T02:29:29.521709Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:29.523851Z","iopub.execute_input":"2024-05-22T02:29:29.524115Z","iopub.status.idle":"2024-05-22T02:29:29.532988Z","shell.execute_reply.started":"2024-05-22T02:29:29.524093Z","shell.execute_reply":"2024-05-22T02:29:29.532057Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nhidden_size = 768\nimg_size = 224\n\ndata_dir = '/kaggle/input/skin-disease-train/binary'","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:32.611070Z","iopub.execute_input":"2024-05-22T02:29:32.611441Z","iopub.status.idle":"2024-05-22T02:29:32.616022Z","shell.execute_reply.started":"2024-05-22T02:29:32.611415Z","shell.execute_reply":"2024-05-22T02:29:32.615012Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for folder in os.listdir(data_dir):\n    print(folder, len(os.listdir(os.path.join(data_dir, folder))))","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:33.555661Z","iopub.execute_input":"2024-05-22T02:29:33.556037Z","iopub.status.idle":"2024-05-22T02:29:35.801164Z","shell.execute_reply.started":"2024-05-22T02:29:33.556011Z","shell.execute_reply":"2024-05-22T02:29:35.800079Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"무증상 189376\n유증상 186746\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.transforms import (\n    Compose,\n    ToTensor,\n    RandomHorizontalFlip,\n    RandomVerticalFlip\n)\n\ntransforms = Compose(\n        [\n            ToTensor(),\n            RandomHorizontalFlip(),\n            RandomVerticalFlip()\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:35.802724Z","iopub.execute_input":"2024-05-22T02:29:35.803017Z","iopub.status.idle":"2024-05-22T02:29:35.808086Z","shell.execute_reply.started":"2024-05-22T02:29:35.802992Z","shell.execute_reply":"2024-05-22T02:29:35.807169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = datasets.ImageFolder(data_dir, transforms)\n# train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms)\n# valid_dataset = datasets.ImageFolder(os.path.join(data_dir, 'valid'), transforms)\n# test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:29:36.226948Z","iopub.execute_input":"2024-05-22T02:29:36.227758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_size = int(len(dataset)*0.2)\ntrain_size = len(dataset) - valid_size\n\ntrain_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset), len(valid_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n\ninception_v4 </br> \nresnet50 </br>\nresnetv2_50 </br>\nefficientnet_b0 </br>\nvit_base_resnet50_224_in21k </br>\nvit_base_patch16_224 </br>\nvit_base_patch8_224","metadata":{}},{"cell_type":"code","source":"model_name = 'vit_base_patch8_224'\npretrained = True\nmodel = timm.create_model(model_name, pretrained=pretrained, num_classes=hidden_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_initialized()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_accuracy(X, Y):\n    max_vals, max_indices = torch.max(X, 1)\n    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n    return train_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n\ndef loss_epoch_curve(train_loss_epoch, val_loss_epoch, train_acc_epoch, val_acc_epoch):\n\n  figure, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n  ax[0].plot(train_loss_epoch)\n  ax[0].plot(val_loss_epoch)\n  ax[0].set_title('Loss-Epoch curve')\n  ax[0].set_ylabel('Loss')\n  ax[0].set_xlabel('Epoch')\n  ax[0].legend(['train', 'val'], loc='upper right')\n\n  ax[1].plot(train_acc_epoch)\n  ax[1].plot(val_acc_epoch)\n  ax[1].set_title('Model Accuracy')\n  ax[1].set_ylabel('Accuracy')\n  ax[1].set_xlabel('Epoch')\n  ax[1].legend(['train', 'val'], loc='lower right')\n\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, model, hidden_size, num_classes=2): \n        super().__init__()\n        self.model = model\n        self.classifier = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return self.classifier(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\nimport pickle\n\ndef save_checkpoint(epoch, labels, model, optimizer, \n                    train_loss_epoch, val_loss_epoch, train_acc_epoch, val_acc_epoch,\n                    model_path, filename):\n    \n    # model_dict = OrderedDict([(k, v) for k, v in model.state_dict().items()])\n    state = {\n        'epoch': epoch,\n        'state_dict': model.module.state_dict(), # model_dict,\n        'optimizer': optimizer.state_dict(),\n        'label': labels\n    }\n    torch.save(state, os.path.join(model_path, f'{filename}.pt'))\n\n    config = {\"train\":{\"acc\":train_acc_epoch, \"loss\":train_loss_epoch},\n          \"valid\":{\"acc\":val_acc_epoch, \"loss\":val_loss_epoch}}\n\n    with open(os.path.join(model_path, f'{filename}.pickle'),'wb') as fw:\n        pickle.dump(config, fw)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '/kaggle/working/multi'\nos.makedirs(model_path, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dict(model, optimizer, dict_file):\n    \n    pretrained = torch.load(dict_file)\n    \n    epoch = pretrained['epoch']\n    state_dict = pretrained['state_dict']\n    opt_dict = pretrained['optimizer']\n    labels = pretrained['label']\n    \n    model_dict = model.state_dict()\n    model_dict.update(state_dict)\n    model.load_state_dict(model_dict)\n    \n    optimizer.load_state_dict(opt_dict)\n    \n    return epoch, labels, model, optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\ndef load_records(pkl_file):\n    with open(pkl_file, 'rb') as f:\n        records = pickle.load(f)\n    return records['train'], records['valid']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/dog-skin-multimodel/multi'\n\npretrained = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_dataset.classes\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_worker(model, hidden_size, labels, model_path, model_name, num_epochs, mixed_precision='fp16', batch_size=batch_size):\n    \n    num_epochs = num_epochs\n    # scaler = torch.cuda.amp.GradScaler()\n\n    best_val_acc, best_val_loss = 0.0, 100.0\n\n    train_loss_epoch, val_loss_epoch = [], []\n    train_acc_epoch, val_acc_epoch = [], []\n    epoch_start = 0\n\n    if pretrained:\n        dict_file = os.path.join(file_path, f'{model_name}.pt')\n        pkl_file = os.path.join(file_path, f'{model_name}.pickle')\n\n        epoch_start, labels, model, optimizer = load_dict(model, optimizer, dict_file)\n        train_epoch, valid_epoch = load_records(pkl_file)\n        train_loss_epoch, train_acc_epoch = train_epoch['loss'], train_epoch['acc']\n        val_loss_epoch, val_acc_epoch = valid_epoch['loss'], valid_epoch['acc']\n\n    accelerator = Accelerator(mixed_precision=mixed_precision, log_with='wandb')\n    \n    # train_sampler = DistributedSampler(train_dataset)\n    # valid_sampler = DistributedSampler(valid_dataset)\n    \n    train_loader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=4)\n                              # pin_memory=True,\n                              # sampler=train_sampler)\n\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=batch_size,\n                              shuffle=False,\n                              num_workers=4)\n                              # pin_memory=True,\n                              # sampler=valid_sampler)\n    \n    classifier = Classifier(model, hidden_size, num_classes=6).to(device)\n        \n    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) # correct_bias=False # Adam\n    loss_fn = nn.CrossEntropyLoss()\n    \n    classifier, optimizer, train_loader, valid_loader = accelerator.prepare(\n        classifier, optimizer, train_loader, valid_loader\n    )\n    \n    # num_training_steps=num_epochs* len(train_loader)\n    # progress_bar=tqdm(range(num_training_steps))\n    \n    for e in range(epoch_start+1, num_epochs+epoch_start+1):\n        train_acc, train_loss = 0.0, 0.0\n        val_acc, val_loss = 0.0, 0.0\n        classifier.train()\n        # train_sampler.set_epoch(e)\n        \n        for batch_id, batch in enumerate(tqdm.notebook.tqdm(train_loader)):\n\n            img = batch[0].to(device)\n            label = batch[1].to(device) # .squeeze(1) .float()\n\n            # with torch.cuda.amp.autocast():\n            out = classifier(img).squeeze(1)\n            loss = loss_fn(out, label)\n\n            train_loss += loss.item()\n\n            accelerator.backward(loss) # loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n#             scaler.scale(loss).backward()\n#             scaler.step(optimizer)\n#             scaler.update()\n#             optimizer.zero_grad()\n\n            train_acc += calc_accuracy(out, label)\n\n        tot_train_acc = train_acc / (batch_id+1)\n        mean_train_loss = train_loss / (batch_id+1)\n        train_loss_epoch.append(mean_train_loss)\n        train_acc_epoch.append(tot_train_acc)\n        accelerator.print(\"[GPU {}] epoch {} train acc {} loss {}\".format(accelerator.local_process_index, e, tot_train_acc, mean_train_loss))\n\n        classifier.eval()\n        for batch_id, batch in enumerate(tqdm.notebook.tqdm(valid_loader)):\n\n            img = batch[0].to(device)\n            label = batch[1].to(device) # .squeeze(1)\n            \n            # with torch.cuda.amp.autocast():\n            out = classifier(img).squeeze(1)\n            loss = loss_fn(out, label)\n                \n            val_loss += loss.item()\n            val_acc += calc_accuracy(out, label)\n\n        tot_acc = val_acc / (batch_id+1)\n        mean_val_loss = val_loss / (batch_id+1)\n        val_loss_epoch.append(mean_val_loss)\n        val_acc_epoch.append(tot_acc)\n        accelerator.print(\"[GPU {}] epoch {} valid acc {} loss {}\".format(accelerator.local_process_index, e, tot_acc, mean_val_loss))\n        if accelerator.local_process_index==0 and best_val_loss > mean_val_loss:\n          clear_output(wait=True)\n          print(\"epoch {} train acc {} validation acc {}\".format(e, tot_train_acc, tot_acc))\n          loss_epoch_curve(train_loss_epoch, val_loss_epoch, train_acc_epoch, val_acc_epoch)\n          best_val_loss = mean_val_loss\n          save_checkpoint(e, labels, classifier, optimizer, \n                          train_loss_epoch, val_loss_epoch, train_acc_epoch, val_acc_epoch,\n                          model_path, model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_initialized()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = (model, hidden_size, labels, model_path, model_name, 10, 'fp16', 32)\nnotebook_launcher(main_worker, args, num_processes=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}